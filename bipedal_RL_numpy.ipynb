{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LevxyvAnZ-q3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def clip_gradients(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    \n",
        "    # Compute the norm of the gradients\n",
        "    grad_norm = np.linalg.norm(grads)\n",
        "\n",
        "    # Clip the gradients if the norm is too large\n",
        "    clip_threshold = 1.0\n",
        "    if grad_norm > clip_threshold:\n",
        "        clipped_grads = grads * (clip_threshold / grad_norm)\n",
        "    else:\n",
        "        clipped_grads = grads\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK-CCJHancsn"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This is an two layered neural network\n",
        "when initiating set the number of layers accordingly!!!!!\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class NeuralNet_actor():\n",
        "    def __init__(self,layers,lr,iterations=1000):\n",
        "        self.params={}\n",
        "        self.lr=lr\n",
        "        self.iterations=iterations\n",
        "        self.loss=[]\n",
        "        self.sample_size=None\n",
        "        self.layers=layers\n",
        "        np.random.seed(1)\n",
        "        self.params['w1']=np.random.randn(self.layers[0],self.layers[1])\n",
        "        self.params['b1']=np.random.rand(self.layers[1],)\n",
        "        self.params['w2']=np.random.rand(self.layers[1],self.layers[2])\n",
        "        self.params['b2']=np.random.rand(self.layers[2],)\n",
        "\n",
        "        ''' def initial_weights(self):\n",
        "        np.random.seed(1)\n",
        "        self.params['w1']=np.random.randn(self.layers[0],self.layers[1])\n",
        "        self.params['b1']=np.random.rand(self.layers[1],)\n",
        "        self.params['w2']=np.random.rand(self.layers[1],self.layers[2])\n",
        "        self.params['b2']=np.random.rand(self.layers[2],) '''\n",
        "\n",
        "    def relu(self,z):\n",
        "        return np.maximum(0,z)\n",
        "        \n",
        "\n",
        "    def tanh(self,z):\n",
        "        z=np.float128(z)\n",
        "        return 1/(1+np.exp(-z))\n",
        "    \n",
        "    \n",
        "    def back_propagation(self,loss):\n",
        "        #gradients calculated by loss using backpropagation\n",
        "        dw1=np.gradient(self.params['w1'],loss,axis=0)\n",
        "        dw2=np.gradient(self.params['w2'],loss,axis=0)\n",
        "        db1=np.gradient(self.params['b1'],loss,axis=0)\n",
        "        db2=np.gradient(self.params['b2'],loss,axis=0)\n",
        "        dw1=clip_gradients(dw1,1)\n",
        "        dw2=clip_gradients(dw2,1)\n",
        "        db1=clip_gradients(db1,1)\n",
        "        db2=clip_gradients(db2,1) \n",
        "        \n",
        "\n",
        "        #update the weights and bias\n",
        "        self.params['w1'] = self.params['w1'] - self.lr * dw1\n",
        "        self.params['w2'] = self.params['w2'] - self.lr * dw2\n",
        "        self.params['b1'] = self.params['b1'] - self.lr * db1\n",
        "        self.params['b2'] = self.params['b2'] - self.lr * db2\n",
        "\n",
        "\n",
        "    def train(self,loss):\n",
        "        self.loss_store=[]\n",
        "        #self.initial_weights()\n",
        "        for i in range(self.iterations):\n",
        "            self.back_propagation(loss)\n",
        "            self.loss_store.append(loss)\n",
        "\n",
        "    def predict(self, X):\n",
        "        Z1 = X.dot(self.params['w1']) + self.params['b1']\n",
        "        A1 = self.relu(Z1)\n",
        "        Z2 = A1.dot(self.params['w2']) + self.params['b2']\n",
        "        pred = self.relu(Z2)\n",
        "        return np.round(pred) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JeUNQvbnlI6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This is an two layered neural network\n",
        "when initiating set the number of layers accordingly!!!!!\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "import scipy\n",
        "\n",
        "class NeuralNet_critic():\n",
        "    def __init__(self,layers,lr,iterations=1000):\n",
        "        self.params={}\n",
        "        self.lr=lr\n",
        "        self.iterations=iterations\n",
        "        self.loss=[]\n",
        "        self.sample_size=None\n",
        "        self.layers=layers\n",
        "        np.random.seed(1)\n",
        "        self.params['w1']=np.random.randn(self.layers[0],self.layers[1])\n",
        "        self.params['b1']=np.random.rand(self.layers[1],)\n",
        "        self.params['w2']=np.random.rand(self.layers[1],self.layers[2])\n",
        "        self.params['b2']=np.random.rand(self.layers[2],)\n",
        "\n",
        "        ''' def initial_weights(self):\n",
        "        np.random.seed(1)\n",
        "        self.params['w1']=np.random.randn(self.layers[0],self.layers[1])\n",
        "        self.params['b1']=np.random.rand(self.layers[1],)\n",
        "        self.params['w2']=np.random.rand(self.layers[1],self.layers[2])\n",
        "        self.params['b2']=np.random.rand(self.layers[2],) '''\n",
        "\n",
        "    def relu(self,z):\n",
        "        return np.maximum(0,z)\n",
        "        \n",
        "\n",
        "    def tanh(self,z):\n",
        "        z=np.float128(z)\n",
        "        return 1/(1+np.exp(-z))\n",
        "    \n",
        "    \n",
        "    def back_propagation(self,loss):\n",
        "        #gradients calculated by loss using backpropagation\n",
        "        dw1=np.gradient(self.params['w1'],loss,axis=0)\n",
        "        dw2=np.gradient(self.params['w2'],loss,axis=0)\n",
        "        db1=np.gradient(self.params['b1'],loss,axis=0)\n",
        "        #print(np.shape(self.params['b2']))\n",
        "        #db2=np.gradient(self.params['b2'],loss,axis=0)\n",
        "        dw1=clip_gradients(dw1,1)\n",
        "        dw2=clip_gradients(dw2,1)\n",
        "        db1=clip_gradients(db1,1) \n",
        "        #db2=clip_gradients(db2,1)\n",
        "        #update the weights and bias\n",
        "        self.params['w1'] = self.params['w1'] - self.lr * dw1\n",
        "        self.params['w2'] = self.params['w2'] - self.lr * dw2\n",
        "        self.params['b1'] = self.params['b1'] - self.lr * db1\n",
        "        self.params['b2'] = self.params['b2'] - self.lr\n",
        "\n",
        "\n",
        "    def train(self,loss):\n",
        "        self.loss_store=[]\n",
        "        #self.initial_weights()\n",
        "        for i in range(self.iterations):\n",
        "            self.back_propagation(loss)\n",
        "            self.loss_store.append(loss)\n",
        "\n",
        "    def predict(self, X):\n",
        "        Z1 = X.dot(self.params['w1']) + self.params['b1']\n",
        "        A1 = self.relu(Z1)\n",
        "        Z2 = A1.dot(self.params['w2']) + self.params['b2']\n",
        "        pred = self.relu(Z2)\n",
        "        return np.round(pred) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ourVGAtxnrR7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This is an implementation of proximal policy optimization with actor and critic neural networks\n",
        "'''\n",
        "import numpy as np\n",
        "#from actornn import NeuralNet_actor\n",
        "#from crticnn import NeuralNet_critic\n",
        "from scipy.stats import multivariate_normal\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class store:\n",
        "    def __init__(self):\n",
        "        self.actions=[]\n",
        "        self.states=[]\n",
        "        self.log_probs=[]\n",
        "        self.rewards=[]\n",
        "        self.state_values=[]\n",
        "        self.terminals=[]\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.log_probs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.terminals[:]\n",
        "\n",
        "class actor_critic():\n",
        "    def __init__(self,action_dim,states_dim,action_std,lr_actor,lr_critic):\n",
        "        self.action_dim=action_dim\n",
        "        self.states_dim=states_dim\n",
        "        self.action_std=action_std\n",
        "        self.actor_layers=[self.states_dim,13,self.action_dim]\n",
        "        self.actor=NeuralNet_actor(self.actor_layers,lr_actor)\n",
        "        self.critic_layers=[self.states_dim,13,1]\n",
        "        self.critic=NeuralNet_critic(self.critic_layers,lr_critic)\n",
        "        p=random.uniform(0.3, 0.5)\n",
        "        self.cov=np.full((self.action_dim,),p)\n",
        "        self.cov_mat=np.diag(self.cov)\n",
        "\n",
        "    def action_pred(self,obs):\n",
        "        action_mean=self.actor.predict(obs)\n",
        "        #action=np.random.multivariate_normal(action_mean,self.cov_mat)\n",
        "        action=np.random.multivariate_normal(action_mean,self.cov_mat)\n",
        "        log_prob=multivariate_normal.logpdf(action,action_mean,self.cov_mat)\n",
        "        state_values=self.critic.predict(obs)\n",
        "        return action,log_prob,state_values\n",
        "    \n",
        "    def evaluate(self,obs,action):\n",
        "        value=self.critic.predict(obs)\n",
        "        action_mean=self.actor.predict(obs)\n",
        "        action_mean=np.mean(action_mean,axis=0)\n",
        "        \n",
        "        #distribution=multivariate_normal(action_mean,self.cov_mat)\n",
        "        log_probs=multivariate_normal.logpdf(action,action_mean,self.cov_mat)\n",
        "        return value,log_probs\n",
        "    \n",
        "\n",
        "#implementation of proximal policy optimization\n",
        "\n",
        "class ppo():\n",
        "    def __init__(self,states_dim,action_dim,epoch,gamma,lam,clip,lr_actor,lr_critic,action_std):\n",
        "        self.states_dim=states_dim\n",
        "        self.actor_dim=action_dim\n",
        "        self.epoch=epoch\n",
        "        self.clip=clip\n",
        "        self.gamma=gamma\n",
        "        self.lamda=lam\n",
        "        self.lr_actor=lr_actor\n",
        "        self.lr_critic=lr_critic\n",
        "        self.action_std=action_std\n",
        "        self.local=store()\n",
        "        self.policy=actor_critic(self.actor_dim,self.states_dim,self.action_std,self.lr_actor,self.lr_critic)\n",
        "        \n",
        "    def select_action(self,state):\n",
        "        action,log_probs,state_values=self.policy.action_pred(state)\n",
        "        self.local.actions.append(action)\n",
        "        self.local.log_probs.append(log_probs)\n",
        "        self.local.states.append(state)\n",
        "        self.local.state_values.append(state_values)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def ppo_update(self):\n",
        "        rewards=[]\n",
        "        discounted_reward=0\n",
        "        for reward, terminals in zip(reversed(self.local.rewards), reversed(self.local.terminals)):\n",
        "            if terminals:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        rewards=np.array(rewards)\n",
        "        rewards=(rewards-np.mean(rewards))/(np.std(rewards)+1e7)\n",
        "\n",
        "        old_states=np.squeeze(np.array(self.local.states))\n",
        "        old_actions=np.squeeze(np.array(self.local.actions))\n",
        "        old_state_values=np.squeeze(np.array(self.local.state_values))\n",
        "        old_log_probs=np.squeeze(np.array(self.local.log_probs))\n",
        "\n",
        "        advantages=rewards-old_state_values\n",
        "\n",
        "        for _ in range(self.epoch):\n",
        "            state_values,log_probs=self.policy.evaluate(old_states,old_actions)\n",
        "\n",
        "            log_probs=np.array(log_probs)\n",
        "\n",
        "            #ratio\n",
        "            ratio=np.exp(log_probs-old_log_probs)\n",
        "            surr1 = ratio* advantages\n",
        "            surr2 = np.clip(ratio, 1-self.clip, 1+self.clip) * advantages\n",
        "            \n",
        "            #actor loss\n",
        "            actor_loss=-np.mean(np.minimum(surr1,surr2))\n",
        "            print(\"a\",actor_loss)\n",
        "            #print(\"aaa\",np.shape(actor_loss))\n",
        "            state_values=np.array(state_values)\n",
        "            #critic loss\n",
        "            critic_loss=np.mean(np.square(state_values-rewards))\n",
        "            print(\"aa\",critic_loss)\n",
        "            #print(\"aa\",critic_loss)\n",
        "            #updating the weights of actor neural net\n",
        "            self.policy.actor.train(actor_loss)\n",
        "\n",
        "            #updating the weights of critic neural net\n",
        "            self.policy.critic.train(critic_loss)\n",
        "\n",
        "        self.local.clear()\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3VGyUl2Enxng",
        "outputId": "3162aaa7-5acf-4c45-8905-2f37a04107a6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------\n",
            "Initialising the continuous action state policy\n",
            "training the policy in : BipedalWalker-v3\n",
            "current logger number for BipedalWalker-v3: 0\n",
            "-----------------------------------------------------------------\n",
            "maximum training steps: 3000000\n",
            "maximum episode time: 1000\n",
            "frequency of model saving: 100000\n",
            "logging frequency: 2000\n",
            "showing reward frequency: 10000\n",
            "action dim: 4\n",
            "observations dim: 24\n",
            "action distribution or multivariate normal: 0.6\n",
            "decay rate of action distribution: 0.02\n",
            "decay frequency of action distibution: 250000\n",
            "minimum action distribution: 0.02\n",
            "policy update frequency: 4000\n",
            "epoch: 1\n",
            "policy ratio clip: 0.2\n",
            "discount factor: 0.99\n",
            "actor learning rate: 0.0003\n",
            "critic learning rate: 0.002\n",
            "random seed 1\n",
            "-----------------------------------------------------------------\n",
            "Proximal policy initialisation begins!\n",
            "starting the training time at: 2023-02-17 09:54:01\n",
            "a 22.799068083709507\n",
            "aa 247.15925000000536\n",
            "a 15.876411169414713\n",
            "aa 188.79050000000393\n",
            "Episode:77 \t Timestep:10000 \t Average Reward:-115.93 \t\n",
            "a 13.473270715121929\n",
            "aa 139.36725000000374\n",
            "a 14.384056276780498\n",
            "aa 97.93350000000363\n",
            "a 8.152772396135896\n",
            "aa 64.7635000000045\n",
            "Episode:155 \t Timestep:20000 \t Average Reward:-115.82 \t\n",
            "a 5.988156126175913\n",
            "aa 40.91500000000333\n",
            "a 9.672885028976445\n",
            "aa 24.3965000000039\n",
            "Episode:235 \t Timestep:30000 \t Average Reward:-115.63 \t\n",
            "a 2.8153835671422702\n",
            "aa 11.640250000003498\n",
            "a 2.9695516339412027\n",
            "aa 7.280000000003707\n",
            "a 2.359379443254024\n",
            "aa 9.836750000003896\n",
            "Episode:316 \t Timestep:40000 \t Average Reward:-115.58 \t\n",
            "a 3.6596150310492583\n",
            "aa 12.536750000003957\n",
            "a 2.508448647459424\n",
            "aa 16.686500000003633\n",
            "Episode:398 \t Timestep:50000 \t Average Reward:-115.37 \t\n",
            "a 2.438019680365043\n",
            "aa 18.926250000003314\n",
            "a 2.106354779659649\n",
            "aa 20.052250000003987\n",
            "a 2.0051294239817503\n",
            "aa 22.429250000004476\n",
            "Episode:481 \t Timestep:60000 \t Average Reward:-115.34 \t\n",
            "a 2.576357868814347\n",
            "aa 28.269000000005036\n",
            "a 2.401263066002068\n",
            "aa 30.26025000000487\n",
            "Episode:567 \t Timestep:70000 \t Average Reward:-115.08 \t\n",
            "a 2.3907852029859287\n",
            "aa 31.882750000002748\n",
            "a 2.3803334174214923\n",
            "aa 33.82350000000426\n",
            "a 2.3888782119890837\n",
            "aa 34.32350000000451\n",
            "Episode:654 \t Timestep:80000 \t Average Reward:-114.86 \t\n",
            "a 2.213400642944367\n",
            "aa 36.016250000004675\n",
            "a 2.2928006498564155\n",
            "aa 38.81050000000463\n",
            "Episode:743 \t Timestep:90000 \t Average Reward:-114.8 \t\n",
            "a 2.4916650329279464\n",
            "aa 42.99750000000202\n",
            "a 2.483400646362704\n",
            "aa 44.06975000000375\n",
            "a 2.5392006389466517\n",
            "aa 46.30450000000201\n",
            "Episode:831 \t Timestep:100000 \t Average Reward:-114.84 \t\n",
            "a 2.4930006499673265\n",
            "aa 46.83375000000364\n",
            "a 2.6276006077623926\n",
            "aa 51.14300000000211\n",
            "Episode:920 \t Timestep:110000 \t Average Reward:-114.71 \t\n",
            "a 2.4388006171772623\n",
            "aa 48.68400000000208\n",
            "a 2.4940006363825797\n",
            "aa 51.6670000000019\n",
            "a 2.5682006071806365\n",
            "aa 54.3332500000016\n",
            "Episode:1014 \t Timestep:120000 \t Average Reward:-114.36 \t\n",
            "a 2.524400614768579\n",
            "aa 54.50050000000369\n",
            "a 2.626000632291123\n",
            "aa 58.25850000000159\n",
            "Episode:1109 \t Timestep:130000 \t Average Reward:-114.23 \t\n",
            "a 2.674000598302299\n",
            "aa 60.65800000000152\n",
            "a 2.8334006065527193\n",
            "aa 68.4042500000015\n",
            "a 2.8720006863270573\n",
            "aa 78.59250000000246\n",
            "Episode:1202 \t Timestep:140000 \t Average Reward:-114.83 \t\n",
            "a 2.3712007114367686\n",
            "aa 65.1470000000024\n",
            "a 2.765400613379811\n",
            "aa 78.67025000000228\n",
            "Episode:1304 \t Timestep:150000 \t Average Reward:-116.37 \t\n",
            "a 2.9480005735947348\n",
            "aa 85.92400000000136\n",
            "a 3.5414005561555246\n",
            "aa 111.55675000000109\n",
            "a 4.472400552613018\n",
            "aa 164.05600000000126\n",
            "Episode:1417 \t Timestep:160000 \t Average Reward:-114.92 \t\n",
            "a 3.7704005244044554\n",
            "aa 149.75700000000094\n",
            "a 3.7848005587195557\n",
            "aa 161.62900000000087\n",
            "Episode:1577 \t Timestep:170000 \t Average Reward:-114.04 \t\n",
            "a 1.7920006128343475\n",
            "aa 69.19900000000172\n",
            "a 0.7810006454928798\n",
            "aa 24.32275000000353\n",
            "a 0.21200112956751035\n",
            "aa 6.897000000009594\n",
            "Episode:1681 \t Timestep:180000 \t Average Reward:-113.0 \t\n",
            "a 0.6952009130229349\n",
            "aa 31.790000000007392\n",
            "a 0.5542009884104105\n",
            "aa 26.388250000007496\n",
            "Episode:1716 \t Timestep:190000 \t Average Reward:-98.2 \t\n",
            "a 0.3600008506188132\n",
            "aa 16.127000000007158\n",
            "a 0.11400073828778239\n",
            "aa 3.1460000000064046\n",
            "a 0.24700050657182254\n",
            "aa 21.592750000004095\n",
            "Episode:1740 \t Timestep:200000 \t Average Reward:-97.47 \t\n",
            "a 2.8317645721407e-08\n",
            "aa 1.5625330195015463e-14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1080: RuntimeWarning: overflow encountered in true_divide\n",
            "  out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)\n",
            "<ipython-input-27-5570ce58ad9e>:12: RuntimeWarning: invalid value encountered in multiply\n",
            "  clipped_grads = grads * (clip_threshold / grad_norm)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1101: RuntimeWarning: overflow encountered in true_divide\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1108: RuntimeWarning: overflow encountered in true_divide\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1101: RuntimeWarning: overflow encountered in double_scalars\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1108: RuntimeWarning: overflow encountered in double_scalars\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n",
            "<ipython-input-28-8ec1c77d9bb7>:52: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.params['w1'] = self.params['w1'] - self.lr * dw1\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1080: RuntimeWarning: invalid value encountered in subtract\n",
            "  out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1101: RuntimeWarning: invalid value encountered in subtract\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1108: RuntimeWarning: invalid value encountered in subtract\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1101: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:1108: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n",
            "<ipython-input-28-8ec1c77d9bb7>:53: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.params['w2'] = self.params['w2'] - self.lr * dw2\n",
            "<ipython-input-28-8ec1c77d9bb7>:54: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.params['b1'] = self.params['b1'] - self.lr * db1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0095026d7fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-0095026d7fc6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m#print(\"aaa\",action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m#saving the rewards and terminals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mLIDAR_RANGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             )\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRayCast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         state = [\n",
            "\u001b[0;31mAssertionError\u001b[0m: r.LengthSquared() > 0.0f"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This code is for training the policy\n",
        "Considering only continuous action spaces\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "import gym\n",
        "import csv\n",
        "#from ppo import ppo\n",
        "#from ppo import policy\n",
        "'''\n",
        "for the first training we will be using cartpole_v1\n",
        "'''\n",
        "def train():\n",
        "    #intitiating the environment parameters for training\n",
        "    env_name=\"BipedalWalker-v3\"\n",
        "\n",
        "    #max timesteps in an episode\n",
        "    max_episode_len=1500\n",
        "    #max timesteps in one training\n",
        "    max_training_time=int(4e6)\n",
        "\n",
        "    #printing reward frequency\n",
        "    frequency_print=max_episode_len*10\n",
        "    frequency_log=max_episode_len*2\n",
        "\n",
        "    #action distribution particularly multivariate normal are \n",
        "    #being used for finding the probability of an action in states\n",
        "    action_dist=0.6\n",
        "\n",
        "    #below are the parameters for proximal policy optimization\n",
        "    update_policy_step=max_episode_len*4\n",
        "    #here the policy is being updated one time for epoch times\n",
        "    epoch=1\n",
        "    #clip is the ratio of new_policy upon the old_policy \n",
        "    clip=0.2\n",
        "    #gamma is the discount factor\n",
        "    gamma=0.99 \n",
        "    lam=0.95\n",
        "    #learning rates for actor and critic\n",
        "    lr_actor=0.0004\n",
        "    lr_critic=0.03\n",
        "\n",
        "    #random seed for starting action\n",
        "    random_seed=1\n",
        "    #parameters end here\n",
        "    print(\"Initialising the continuous action state policy\")\n",
        "    print(\"training the policy in : \" +env_name)\n",
        "    #inittaiting the gym\n",
        "    env=gym.make(env_name)\n",
        "\n",
        "    #getting the dimensions of observation of agent in environment\n",
        "    state_dim=env.observation_space.shape[0]\n",
        "\n",
        "    #getting the dimensions of action space of agent\n",
        "    #action_dim=env.action_space.shape[0]\n",
        "    #use above command if the action dim is greater than 1\n",
        "    action_dim=env.action_space.shape[0]\n",
        "\n",
        "    #now logging the policy and rewards in csv files\n",
        "\n",
        "    log_dir=\"ppo_logs\"\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    log_dir=log_dir+'/'+env_name+'/'\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    #look for this command online\n",
        "    num=0\n",
        "    current_num=next(os.walk(log_dir))[2]\n",
        "    num=len(current_num)\n",
        "\n",
        "    #creating the logger file for each lenght of episode\n",
        "    logger_name=env_name+str(num)+\".csv\"\n",
        "    print(\"current logger number for \" +env_name+\":\",num)\n",
        "\n",
        "    #starting the original training\n",
        "    run_number=0 #we need to comstantly change this number as we train \n",
        "    #saving the data\n",
        "    \n",
        "\n",
        "    #printing all the data\n",
        "    print(\"maximum training steps:\",max_training_time)\n",
        "    print(\"maximum episode time:\",max_episode_len)\n",
        "    print(\"frequency of model saving:\",model_save_freq)\n",
        "    print(\"logging frequency:\",frequency_log)\n",
        "    print(\"showing reward frequency:\",frequency_print)\n",
        "    print(\"action dim:\",action_dim)\n",
        "    print(\"observations dim:\",state_dim)\n",
        "    print(\"action distribution or multivariate normal:\",action_dist)\n",
        "    print(\"decay rate of action distribution:\",action_dist_decay_rate)\n",
        "    print(\"decay frequency of action distibution:\",action_dist_decay_freq)\n",
        "    print(\"minimum action distribution:\",min_action_dist)\n",
        "    print(\"policy update frequency:\",update_policy_step)\n",
        "    print(\"epoch:\",epoch)\n",
        "    print(\"policy ratio clip:\",clip)\n",
        "    print(\"discount factor:\",gamma)\n",
        "    print(\"actor learning rate:\",lr_actor)\n",
        "    print(\"critic learning rate:\",lr_critic)\n",
        "    if random_seed:\n",
        "        print(\"random seed\",random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    print(\"Proximal policy initialisation begins!\")\n",
        "    #here we will initialise the policy of agent\n",
        "    ppo_agent=ppo(state_dim,action_dim,epoch,gamma,0.95,0.2,lr_actor,lr_critic,action_dist)\n",
        "    #begin time\n",
        "    start_time=datetime.now().replace(microsecond=0)\n",
        "    print(\"starting the training time at:\",start_time)\n",
        "    log=open(logger_name,\"w+\")\n",
        "    log.write('episode,time,reward\\n')\n",
        "\n",
        "    present_reward=0\n",
        "    present_episode=0\n",
        "    run_log_reward=0\n",
        "    run_log_episodes=0\n",
        "\n",
        "    time_step=0\n",
        "    i_episode=0\n",
        "    \n",
        "    #training begins\n",
        "    while time_step<max_training_time:\n",
        "        state=env.reset()\n",
        "        #it=np.array(state)\n",
        "        #bit=it.reshape(96,-1)\n",
        "        #print(np.shape(state))\n",
        "        #print(np.shape(bit))\n",
        "        current_episode_reward=0\n",
        "        for t in range(1,max_episode_len+1):\n",
        "            #here the action is being selected using the policy\n",
        "            action=ppo_agent.select_action(state)\n",
        "            #print(\"aaa\",action)\n",
        "            state,reward,done,_=env.step(action)\n",
        "            \n",
        "            #saving the rewards and terminals\n",
        "            #ppo_agent.store.rewards.append(reward)\n",
        "            #ppo_agent.store.terminals.append(done)\n",
        "            ppo_agent.local.rewards.append(reward)\n",
        "            ppo_agent.local.terminals.append(done)\n",
        "\n",
        "            time_step+=1\n",
        "            current_episode_reward+=reward\n",
        "\n",
        "            #update the ppo agent\n",
        "            #test15 updating the ppo every time\n",
        "            if time_step % frequency_print ==0:\n",
        "                present_reward_avg=present_reward/present_episode\n",
        "                present_reward_avg=round(present_reward_avg,2)\n",
        "\n",
        "                print(\"Episode:{} \\t Timestep:{} \\t Average Reward:{} \\t\".format(i_episode,time_step,present_reward_avg))\n",
        "                present_reward=0\n",
        "                present_episode=0\n",
        "\n",
        "\n",
        "            if time_step %update_policy_step==0:\n",
        "                ppo_agent.ppo_update()\n",
        "\n",
        "                \n",
        "\n",
        "            if time_step % frequency_log==0:\n",
        "                avg_log_reward=run_log_reward/run_log_episodes\n",
        "                avg_log_reward=round(avg_log_reward,4)\n",
        "\n",
        "                log.write('{},{},{}\\n'.format(i_episode,time_step,avg_log_reward))\n",
        "                log.flush()\n",
        "\n",
        "                run_log_reward=0\n",
        "                run_log_episodes=0\n",
        "\n",
        "            \n",
        "\n",
        "            \n",
        "\n",
        "            #ppo_agent.delete()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        present_reward+=current_episode_reward\n",
        "        present_episode+=1\n",
        "        run_log_reward+=current_episode_reward\n",
        "        run_log_episodes+=1\n",
        "\n",
        "        i_episode+=1\n",
        "\n",
        "    log.close()\n",
        "    env.close\n",
        "\n",
        "    print(\"-----------------------------------------------------------------\")\n",
        "    end_time=datetime.now().replace(microsecond=0)\n",
        "    print(\"started training at\",start_time)\n",
        "    print(\"ended training at:\",end_time)\n",
        "    print(\"total training time:\",end_time-start_time)\n",
        "    print(\"-----------------------------------------------------------------\")\n",
        "    \n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}